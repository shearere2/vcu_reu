# -*- coding: utf-8 -*-
"""DataCleansing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mAnILTWKeIbszbAtW_qV-y4hAjnnPn3h
"""

# Importing pandas for merging datasets
import pandas as pd

# Reading in our 3 datasets: granular, tree, and one with binarized gpa
# calculated on R by Ninel
gran = pd.read_csv('StudentData_granular.csv')
tree = pd.read_csv('StudentData_tree.csv')
gpa_bin = pd.read_csv('gpa_binarized.csv')

# Only using mental health related variables from granular dataset selected
# by Jason
gran = gran[['acpers', 'colearn', 'hours', 'covid_mh', 'covid_iso', 'covid_edu',
         'ptsd_score', 'trauma_sum', 'stress', 'anxiety', 'depress', 'mh_scale',
         'ss_friend', 'ss_family', 'ss_sigother', 'ss_community', 'record_id']]

# Make tree have binarized gpa
tree = pd.merge(left=tree, right=gpa_bin, how='left', on='X')

# Drop other gpa columns, those will be unneeded, along with a few other useless
tree = tree.drop(columns=['gpa_3', 'Unnamed: 0', 'X.1',
                          'gpa_2', 'gpa']).rename(columns={'Grades_Binary':'gpa',
                                                           'X':'record_id'})

# Another way to binarize gpa: 1 if < 3.5 else 0
# df['gpa_2'] = df['gpa_2'].apply(lambda x: 0 if x=='3.51-4.0' else 1)

# Merge granular and tree data on the shared column record_id (participant #)
df = pd.merge(left=tree, right=gran, how='left', on='record_id')

df = df.set_index('record_id')

df # Merged dataframe with all appropriate information

# Prints all the current columns in the dataframe
# We need to decide if we use all of these or just some
for name in df.columns:
  print(name)

#df.to_csv('StudentData_MHTree.csv') #used to export the file

"""6/27/24 Needed to make a few files with different thresholds for GPA"""

import pandas as pd
df = pd.read_csv('StudentData_tree.csv')

def binarize_grades(c):
  if c[0] == 'B': return 1
  return 0

df['gpa'] = df['gpa_3'].apply(binarize_grades)

df = df.rename(columns={'X':'record_id'}).drop(columns=['X.1', 'Unnamed: 0'])

temp = df
df = pd.read_csv('numeric_imputed.csv')

df['gpa'] = temp['gpa']

comments = pd.read_csv('comments.csv')

comments = comments.fillna('')

def penn_to_wn(tag):

    # Convert between the PennTreebank tags to simple Wordnet tags
    if tag.startswith('J'):
        return wn.ADJ
    elif tag.startswith('N'):
        return wn.NOUN
    elif tag.startswith('R'):
        return wn.ADV
    elif tag.startswith('V'):
        return wn.VERB
    return None

from nltk.sentiment import sentiment_analyzer
from nltk.tag import pos_tag
from nltk.corpus import wordnet as wn
from nltk.corpus import sentiwordnet as swn
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('sentiwordnet')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
def analyze_sentiment(sentence:str) -> float:

    # tokenization gets the sentence into a list of words
    token = nltk.word_tokenize(sentence)

    # Gives tags based off type of word (noun, adj, adv, etc.)
    after_tagging = nltk.pos_tag(token)
    sentiment = 0.0
    tokens_count = 0

    # Lemmatizer helps process words to their base form
    # I.e. jumping, jumps, jumpy, ----> jump
    lemmatizer = WordNetLemmatizer()
    for word, tag in after_tagging:
                wn_tag = penn_to_wn(tag)
                if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):
                    continue

                lemma = lemmatizer.lemmatize(word, pos=wn_tag)
                if not lemma:
                    continue

                synsets = wn.synsets(lemma, pos=wn_tag)
                if not synsets:
                    continue
                synset = synsets[0]
                swn_synset = swn.senti_synset(synset.name())

                sentiment += swn_synset.pos_score() - swn_synset.neg_score()
                tokens_count += 1
    # Returns sentiment float standardized on the length of the response.
    return float(sentiment) / (1.0 if len(token)==0 else float(len(token)))

df['connect_numeric'] = comments['comment_connect'].apply(analyze_sentiment)
df['else_numeric'] = comments['comment_else'].apply(analyze_sentiment)
df['mh_numeric'] = comments['comment_mental_health'].apply(analyze_sentiment)
df['miss_numeric'] = comments['comment_miss'].apply(analyze_sentiment)
df['school_numeric'] = comments['comment_school'].apply(analyze_sentiment)
df['surprise_numeric'] = comments['comment_surprise'].apply(analyze_sentiment)

# df.to_csv('numeric_imputed.csv')

